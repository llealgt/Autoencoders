{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising\n",
    "\n",
    "Autoencoders can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1. We'll use noisy images as input and the original, clean images as targets. Here's an example of the noisy images I generated and the denoised images.\n",
    "\n",
    "![Denoising autoencoder](assets/denoising.png)\n",
    "\n",
    "\n",
    "Since this is a harder problem for the network, we'll want to use deeper convolutional layers here, more feature maps. I suggest something like 32-32-16 for the depths of the convolutional layers in the encoder, and the same depths going backward through the decoder. Otherwise the architecture is the same as the convolutional autoencoder.\n",
    "\n",
    "> **Exercise:** Build the network for the denoising autoencoder. It's the same as before, but with deeper layers. I suggest 32-32-16 for the depths, but you can play with these numbers, or add more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f381eb75b38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwy8VZfilogy/VJThl4r6f9/eEE4pKxLvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3822b03a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[3]\n",
    "plt.imshow(img.reshape((28, 28)), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n",
    "kernel_dimension = 5\n",
    "pool_dimension = 2\n",
    "pool_stride = 2\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_,32 ,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 28x28x32\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1,(pool_dimension,pool_dimension),(pool_stride,pool_stride),padding='same')\n",
    "# Now 14x14x32\n",
    "conv2 = tf.layers.conv2d(maxpool1,32,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 14x14x32\n",
    "maxpool2 =  tf.layers.max_pooling2d(conv2,(pool_dimension,pool_dimension),(pool_stride,pool_stride),padding='same')\n",
    "# Now 7x7x32\n",
    "conv3 = tf.layers.conv2d(maxpool2,16,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 7x7x16\n",
    "encoded = tf.layers.max_pooling2d(conv3,(pool_dimension,pool_dimension),(pool_stride,pool_stride),padding='same')\n",
    "# Now 4x4x16\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded,(7,7))\n",
    "# Now 7x7x16\n",
    "conv4 = tf.layers.conv2d(upsample1,16,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 7x7x16\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4,(14,14))\n",
    "# Now 14x14x16\n",
    "conv5 = tf.layers.conv2d(upsample2,32,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 14x14x32\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5,(28,28))\n",
    "# Now 28x28x32\n",
    "conv6 = tf.layers.conv2d(upsample3,32,(kernel_dimension,kernel_dimension),padding='same',activation=tf.nn.relu)\n",
    "# Now 28x28x32\n",
    "\n",
    "logits = tf.layers.conv2d(conv6,1,(kernel_dimension,kernel_dimension),padding='same',activation=None)\n",
    "#Now 28x28x1\n",
    "\n",
    "# Pass logits through sigmoid to get reconstructed image\n",
    "decoded = tf.nn.sigmoid(logits)\n",
    "\n",
    "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=targets_)\n",
    "\n",
    "# Get cost and define the optimizer\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Training loss: 0.6892\n",
      "Epoch: 1/100... Training loss: 0.2068\n",
      "Epoch: 1/100... Training loss: 0.1672\n",
      "Epoch: 2/100... Training loss: 0.1418\n",
      "Epoch: 2/100... Training loss: 0.1363\n",
      "Epoch: 2/100... Training loss: 0.1286\n",
      "Epoch: 3/100... Training loss: 0.1220\n",
      "Epoch: 3/100... Training loss: 0.1210\n",
      "Epoch: 3/100... Training loss: 0.1201\n",
      "Epoch: 4/100... Training loss: 0.1207\n",
      "Epoch: 4/100... Training loss: 0.1172\n",
      "Epoch: 4/100... Training loss: 0.1131\n",
      "Epoch: 5/100... Training loss: 0.1157\n",
      "Epoch: 5/100... Training loss: 0.1147\n",
      "Epoch: 5/100... Training loss: 0.1121\n",
      "Epoch: 6/100... Training loss: 0.1089\n",
      "Epoch: 6/100... Training loss: 0.1079\n",
      "Epoch: 6/100... Training loss: 0.1106\n",
      "Epoch: 7/100... Training loss: 0.1066\n",
      "Epoch: 7/100... Training loss: 0.1073\n",
      "Epoch: 7/100... Training loss: 0.1053\n",
      "Epoch: 8/100... Training loss: 0.1066\n",
      "Epoch: 8/100... Training loss: 0.1036\n",
      "Epoch: 8/100... Training loss: 0.1057\n",
      "Epoch: 9/100... Training loss: 0.1047\n",
      "Epoch: 9/100... Training loss: 0.1036\n",
      "Epoch: 9/100... Training loss: 0.1034\n",
      "Epoch: 10/100... Training loss: 0.1030\n",
      "Epoch: 10/100... Training loss: 0.1030\n",
      "Epoch: 10/100... Training loss: 0.1019\n",
      "Epoch: 11/100... Training loss: 0.0991\n",
      "Epoch: 11/100... Training loss: 0.1022\n",
      "Epoch: 11/100... Training loss: 0.1002\n",
      "Epoch: 12/100... Training loss: 0.1012\n",
      "Epoch: 12/100... Training loss: 0.0973\n",
      "Epoch: 12/100... Training loss: 0.1012\n",
      "Epoch: 13/100... Training loss: 0.1037\n",
      "Epoch: 13/100... Training loss: 0.1046\n",
      "Epoch: 13/100... Training loss: 0.1044\n",
      "Epoch: 14/100... Training loss: 0.1001\n",
      "Epoch: 14/100... Training loss: 0.0993\n",
      "Epoch: 14/100... Training loss: 0.1039\n",
      "Epoch: 15/100... Training loss: 0.1038\n",
      "Epoch: 15/100... Training loss: 0.0994\n",
      "Epoch: 15/100... Training loss: 0.0981\n",
      "Epoch: 16/100... Training loss: 0.0987\n",
      "Epoch: 16/100... Training loss: 0.1012\n",
      "Epoch: 16/100... Training loss: 0.0949\n",
      "Epoch: 17/100... Training loss: 0.1004\n",
      "Epoch: 17/100... Training loss: 0.1025\n",
      "Epoch: 17/100... Training loss: 0.0991\n",
      "Epoch: 18/100... Training loss: 0.0986\n",
      "Epoch: 18/100... Training loss: 0.0972\n",
      "Epoch: 18/100... Training loss: 0.1016\n",
      "Epoch: 19/100... Training loss: 0.0976\n",
      "Epoch: 19/100... Training loss: 0.0968\n",
      "Epoch: 19/100... Training loss: 0.0980\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 200\n",
    "step_counter = 0\n",
    "# Set's how much noise we're adding to the MNIST images\n",
    "noise_factor = 0.5\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(mnist.train.num_examples//batch_size):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        # Get images from the batch\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        \n",
    "        # Add random noise to the input images\n",
    "        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n",
    "        # Clip the images to be between 0 and 1\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        \n",
    "        # Noisy images as inputs, original images as targets\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,\n",
    "                                                         targets_: imgs})\n",
    "\n",
    "        if step_counter%100 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))\n",
    "        step_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
